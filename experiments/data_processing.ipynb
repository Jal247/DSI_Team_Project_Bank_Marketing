{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration:\n",
    "\n",
    "   # Step 1: Data Collection\n",
    "\n",
    "Goal: Gather data from various sources (databases, files, APIs, etc.).\n",
    "\n",
    "Key Actions:\n",
    "\n",
    "Identify and access relevant data sources.\n",
    "Ensure data formats are compatible with processing tools.\n",
    "\n",
    "Output: data collected from the given link which are in the structured CSV format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd  # pandas library\n",
    "import numpy as np   # numphy library\n",
    "import random        # random generation of number\n",
    "import os            # to see the path of the current file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JalpaZenisha\\DSI\\DSI_Team_Project_Bank_Marketing\\experiments\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd()) # displays the current path of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
      "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
      "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
      "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
      "4  unknown    5   may       198         1     -1         0  unknown  no  \n",
      "(45211, 17)\n",
      "   age          job  marital  education default  balance housing loan  \\\n",
      "0   30   unemployed  married    primary      no     1787      no   no   \n",
      "1   33     services  married  secondary      no     4789     yes  yes   \n",
      "2   35   management   single   tertiary      no     1350     yes   no   \n",
      "3   30   management  married   tertiary      no     1476     yes  yes   \n",
      "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
      "\n",
      "    contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  cellular   19   oct        79         1     -1         0  unknown  no  \n",
      "1  cellular   11   may       220         1    339         4  failure  no  \n",
      "2  cellular   16   apr       185         1    330         1  failure  no  \n",
      "3   unknown    3   jun       199         4     -1         0  unknown  no  \n",
      "4   unknown    5   may       226         1     -1         0  unknown  no  \n",
      "(4521, 17)\n"
     ]
    }
   ],
   "source": [
    "#Reading Dataset\n",
    "df = pd.read_csv('../data/raw/bank-full.csv',sep=';')  # reading the full dataset of bank\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "# Reading the subset of full bank dataset\n",
    "df_sub = pd.read_csv('../data/raw/bank.csv',sep=';')  # reading the subset(Sampling) dataset of bank\n",
    "print(df_sub.head())\n",
    "print(df_sub.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is related with direct marketing campaigns of a Portuguese banking institution. \n",
    "   The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, \n",
    "   in order to access if the product (bank term deposit) would be (or not) subscribed. \n",
    "\n",
    "   There are two datasets: \n",
    "\n",
    "      1) bank-full.csv with all examples (45211), ordered by date (from May 2008 to November 2010).\n",
    "      2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv.\n",
    "      \n",
    "   The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).\n",
    "\n",
    "   The classification goal is to predict if the client will subscribe a term deposit (variable y).\n",
    "\n",
    "1. Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)\n",
    "\n",
    "2. Number of Attributes: 16 + output attribute.\n",
    "\n",
    "3. Attribute information:\n",
    "\n",
    "   Input variables:\n",
    "   \n",
    "**bank client data:**\n",
    "\n",
    "   1 - age (numeric)\n",
    "\n",
    "   2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\n",
    "                                       \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \n",
    "\n",
    "   3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n",
    "   \n",
    "   4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n",
    "\n",
    "   5 - default: has credit in default? (binary: \"yes\",\"no\")\n",
    "\n",
    "   6 - balance: average yearly balance, in euros (numeric)\n",
    "\n",
    "   7 - housing: has housing loan? (binary: \"yes\",\"no\")\n",
    "\n",
    "   8 - loan: has personal loan? (binary: \"yes\",\"no\")\n",
    "\n",
    "   **related with the last contact of the current campaign:**\n",
    "\n",
    "   9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n",
    "\n",
    "   10 - day: last contact day of the month (numeric)\n",
    "\n",
    "   11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\n",
    "\n",
    "   12 - duration: last contact duration, in seconds (numeric)\n",
    "\n",
    "   **other attributes:**\n",
    "\n",
    "   13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "\n",
    "   14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n",
    "\n",
    "   15 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "\n",
    "   16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n",
    "  \n",
    "\n",
    "  Output variable (desired target):\n",
    "   17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n",
    "\n",
    "4. Missing Attribute Values: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   age        45211 non-null  int64 \n",
      " 1   job        45211 non-null  object\n",
      " 2   marital    45211 non-null  object\n",
      " 3   education  45211 non-null  object\n",
      " 4   default    45211 non-null  object\n",
      " 5   balance    45211 non-null  int64 \n",
      " 6   housing    45211 non-null  object\n",
      " 7   loan       45211 non-null  object\n",
      " 8   contact    45211 non-null  object\n",
      " 9   day        45211 non-null  int64 \n",
      " 10  month      45211 non-null  object\n",
      " 11  duration   45211 non-null  int64 \n",
      " 12  campaign   45211 non-null  int64 \n",
      " 13  pdays      45211 non-null  int64 \n",
      " 14  previous   45211 non-null  int64 \n",
      " 15  poutcome   45211 non-null  object\n",
      " 16  y          45211 non-null  object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Exploring the data completeness and data type of our variables\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Observations:**\n",
    "\n",
    "The datasets contain customer information, including demographics, account details, and outcomes of bank marketing campaigns.\n",
    "\n",
    "All columns contain 45,211 non-null values, meaning there are no missing values in any column.\n",
    "This ensures data completeness, which is a positive aspect since no immediate imputation or removal of rows is necessary.\n",
    "\n",
    "Data types appear consistent, with a mix of integers and categorical (object) data.\n",
    "The target variable (y) indicates whether a customer subscribed to a term deposit.\n",
    "\n",
    "7 columns are of data type int64 (integer).\n",
    "These columns are numerical and likely represent continuous or discrete variables.\n",
    "\n",
    "10 columns are of data type object.\n",
    "These are categorical variables and may need to be encoded into numerical values if we are going to use them as features in our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "      <td>45211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.936210</td>\n",
       "      <td>1362.272058</td>\n",
       "      <td>15.806419</td>\n",
       "      <td>258.163080</td>\n",
       "      <td>2.763841</td>\n",
       "      <td>40.197828</td>\n",
       "      <td>0.580323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.618762</td>\n",
       "      <td>3044.765829</td>\n",
       "      <td>8.322476</td>\n",
       "      <td>257.527812</td>\n",
       "      <td>3.098021</td>\n",
       "      <td>100.128746</td>\n",
       "      <td>2.303441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>-8019.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>102127.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>4918.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>275.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        balance           day      duration      campaign  \\\n",
       "count  45211.000000   45211.000000  45211.000000  45211.000000  45211.000000   \n",
       "mean      40.936210    1362.272058     15.806419    258.163080      2.763841   \n",
       "std       10.618762    3044.765829      8.322476    257.527812      3.098021   \n",
       "min       18.000000   -8019.000000      1.000000      0.000000      1.000000   \n",
       "25%       33.000000      72.000000      8.000000    103.000000      1.000000   \n",
       "50%       39.000000     448.000000     16.000000    180.000000      2.000000   \n",
       "75%       48.000000    1428.000000     21.000000    319.000000      3.000000   \n",
       "max       95.000000  102127.000000     31.000000   4918.000000     63.000000   \n",
       "\n",
       "              pdays      previous  \n",
       "count  45211.000000  45211.000000  \n",
       "mean      40.197828      0.580323  \n",
       "std      100.128746      2.303441  \n",
       "min       -1.000000      0.000000  \n",
       "25%       -1.000000      0.000000  \n",
       "50%       -1.000000      0.000000  \n",
       "75%       -1.000000      0.000000  \n",
       "max      871.000000    275.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get descriptive statistics for numerical columns\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights from the Data (statistics output)**\n",
    "\n",
    "Age: The average client age is 40.936 = 41 years, with a range from 18(min) to 95(max).\n",
    "\n",
    "Balance: The average balance is €1362, with a large variability (std = €3044), with extreme (possible outliers) values -8019 euro and 102127 euro.\n",
    "\n",
    "Duration: The average call duration is 258 seconds (approx 4 minutes).\n",
    "\n",
    "Campaign: Most clients were contacted on an average of 3 times, between 1 and 3 times most of the clients. with extreme (possible outlier) value of 63 times.\n",
    "\n",
    "Previous: Most clients were contacted first time for this campaign. with extreme(possible outlier) value 275 times.\n",
    "\n",
    "Below are the Possible Outliers: (to investigate)\n",
    "\n",
    "Balance: A minimum balance of -€8019 and a maximum balance of €102,127, indicating a few extreme values.\n",
    "\n",
    "Duration: Calls as short as 0 seconds and as long as 4918 seconds ( approx 82 minutes), suggesting potential outliers.\n",
    "\n",
    "Campaign: Some clients were contacted up to 63 times, which is unusually high.\n",
    "\n",
    "Previous: some clients were contacted 275 times previously before this campaign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Step 2: Data Cleaning and Transformation:\n",
    "\n",
    "The initial step involves cleaning and preparing the raw data to remove inconsistencies, handle missing values, and make the dataset ready for analysis. \n",
    "\n",
    "1. Check for Missing Values: Ensure there are no null values.\n",
    "2. Handle Duplicates: Identify and remove any duplicate rows.\n",
    "3. Standardize Data: Ensure consistent formats for categorical data (e.g., month values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in both datasets\n",
    "missing = df.isnull().sum()\n",
    "missing_sub = df_sub.isnull().sum()\n",
    "\n",
    "# Identify duplicate rows: Check for duplicates in both datasets\n",
    "duplicates = df.duplicated().sum()\n",
    "duplicates_sub = df_sub.duplicated().sum()\n",
    "\n",
    "missing, missing_sub, duplicates, duplicates_sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are no missing values in our data. However, if we look at the categorical data there are 'unknown' values which we can consider as 'missing' data.\n",
    "\n",
    "Also, there are no duplicates rows in our data.\n",
    "We will follow below steps to handle 'unknown' values from \"job', 'education','contact' and 'poutcome' columns.\n",
    "First, calculate the percentage of \"unknown\" values in each column to understand the extent of the issue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected only categorical columns\n",
    "\n",
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']\n",
    "\n",
    "# count unknown values in each column\n",
    "\n",
    "unknown_counts = df[categorical_columns].apply(lambda col: (col == \"unknown\").sum())\n",
    "\n",
    "print(unknown_counts) # the output will give unknown counts for each categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the percentage of 'unknown' values in each relevant column\n",
    "columns_with_unknowns = ['job', 'education', 'poutcome', 'contact']\n",
    "for col in columns_with_unknowns:\n",
    "    unknown_count = df[df[col] == 'unknown'].shape[0]\n",
    "    total_count = df.shape[0]\n",
    "    percentage = (unknown_count / total_count) * 100\n",
    "    print(f\"Column: {col}, Unknown Values: {unknown_count}, Percentage: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**job and education:**  These are likely important features for predicting the target.\n",
    "the percentage of unknown values in 'job' and 'education' are 0.64% and 4.11%. which are very low.\n",
    "\n",
    "We will replace 'unknown' rows with mode value of that column.\n",
    "\n",
    "**contact and poutcome:**\n",
    "\n",
    "These columns have a high proportion (>30%) of \"unknown\" values and if their **impact** on the target variable (y) seems minimal, then we can remove them entirely.\n",
    "\n",
    "Let's check impact of 'contact' and 'potcome' on the target variable(y).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Chi-Square Test (Categorical Association)**\n",
    "\n",
    "A chi-square test can help determine whether there is a statistically significant association between the column (contact or poutcome) and the target variable (y).\n",
    "\n",
    "Interpretation:\n",
    "A p-value < 0.05 indicates a statistically significant relationship between the column and the target variable (y).\n",
    "A high p-value (>0.05) suggests the column has little impact on the target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency tables for 'contact' and 'poutcome'\n",
    "contact_table = pd.crosstab(df['contact'], df['y'])\n",
    "poutcome_table = pd.crosstab(df['poutcome'], df['y'])\n",
    "\n",
    "# Perform chi-square test\n",
    "contact_chi2, contact_p, _, _ = chi2_contingency(contact_table)\n",
    "poutcome_chi2, poutcome_p, _, _ = chi2_contingency(poutcome_table)\n",
    "\n",
    "print(f\"Contact - Chi-square p-value: {contact_p}\")\n",
    "print(f\"Poutcome - Chi-square p-value: {poutcome_p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_p \n",
    "\n",
    "if contact_p >= 0.05:\n",
    "    print(\"there is no association\")\n",
    "else:\n",
    "    print(\"there is significant association\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poutcome_p \n",
    "\n",
    "if poutcome_p >= 0.05:\n",
    "    print(\"there is no association\")\n",
    "else:\n",
    "    print(\"there is significant association\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: we will retain the 'contact' and 'poutcome' columns from the data df. \n",
    "We have two options: \n",
    "1. continue as is with 'unkown' values\n",
    "2. replace 'unknown' values by mode value of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding mode for 'contact' and 'potcome'\n",
    "mode_contact = df['contact'].mode()[0]\n",
    "mode_poutcome = df['poutcome'].mode()[0]\n",
    "\n",
    "print(\"mode value of contact : \", mode_contact)\n",
    "print(\"mode value of poutcome: \", mode_poutcome)\n",
    "\n",
    "# finding mode for 'job' and 'education'\n",
    "mode_job = df['job'].mode()[0]\n",
    "mode_education = df['education'].mode()[0]\n",
    "print(\"mode value of job : \", mode_job)\n",
    "print(\"mode value of education: \", mode_education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 'unknow' values by relevant column's mode value\n",
    "df_cleaned=df\n",
    "for col in ['job', 'education','contact','poutcome']:\n",
    "    mode_value = df[col].mode()[0]\n",
    "    df_cleaned[col] = df_cleaned[col].replace('unknown',mode_value)\n",
    "    print(df_cleaned[col])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking for missing and duplicate values our dataset is df_cleaned ready for Exploratory Data Analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Bar Plot\n",
    "# Shows the distribution of one categorical variable grouped by another.\n",
    "# Use Case: Compare education grouped by housing.\n",
    "\n",
    "sns.countplot(x='poutcome', hue='y', data=df)\n",
    "plt.title(\"Relation of poutcome and Subscription\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Case: Analyze columns like job, education, marital, etc.\n",
    "\n",
    "# Example for the 'job' column\n",
    "sns.countplot(x='marital', data=df)\n",
    "plt.xticks(rotation=45)  # Rotate labels for readability\n",
    "plt.title(\"Distribution of marital Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Bar Plot\n",
    "# Shows the distribution of one categorical variable across another.\n",
    "# Use Case: Compare education across y (outcome of campaign).\n",
    "\n",
    "pd.crosstab(df['education'], df['y']).plot(kind='bar', stacked=True)\n",
    "plt.title(\"Education vs. Campaign Outcome\")\n",
    "#plt.xlabel('Education')\n",
    "#plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  line plot for 'poutcome' vs output variable y.\n",
    "sns.countplot(data=df, x='poutcome', hue='y')\n",
    "plt.title(\"poutcome vs Subscription\")\n",
    "#plt.xticks(np.arange(0, 100, step=10))  # Set label locations.\n",
    "#plt.xticks(0,10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  line plot for 'balance'. Helps to identify the outliers or skewed distribution\n",
    "sns.countplot(data=df, x='contact', hue='y')\n",
    "plt.title(\"contact vs Subscription\")\n",
    "#plt.xticks(np.arange(0, 100, step=10))  # Set label locations.\n",
    "#plt.xticks(0,10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize text formatting for categorical columns.**\n",
    "\n",
    "To standardize the categorical data in the bank-full.csv file, follow these steps:\n",
    "\n",
    "1. Separate Numerical and Categorical Columns\n",
    "Start by identifying which columns are numerical and which are categorical.\n",
    "2. Clean and Standardize Categorical Data\n",
    "    (a) Clean Values\n",
    "    Use string operations to ensure consistency:\n",
    "    for col in categorical_columns:\n",
    "        data_full_parsed[col] = data_full_parsed[col].str.lower().str.strip()\n",
    "\n",
    "    (b) Encode Categorical Values\n",
    "    \n",
    "        Depending on the type of categorical data:\n",
    "\n",
    "        1. Label Encoding for ordinal data:\n",
    "\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            for col in categorical_columns:\n",
    "             data_full_parsed[col] = le.fit_transform(data_full_parsed[col])\n",
    "\n",
    "\n",
    "        2. One-Hot Encoding for nominal data:\n",
    "\n",
    "        data_full_parsed = pd.get_dummies(data_full_parsed, columns=categorical_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step:3 Exploratory Data Analysis\n",
    "\n",
    "Goal: Gain insights into the dataset and understand relationships between variables.\n",
    "\n",
    "Actions:\n",
    "Summarize numerical and categorical variables.\n",
    "Visualize distributions, correlations, and trends.\n",
    "Identify outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Visualize balance distribution\n",
    "sns.histplot(cleaned_df['balance'], kde=True)\n",
    "plt.title(\"Balance Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for numeric columns\n",
    "numerical_columns = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "sns.heatmap(cleaned_df[numerical_columns].corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for all columns\n",
    "#numerical_columns = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "col=[contact]\n",
    "sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions (e.g., histograms or boxplots)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram for age \n",
    "sns.histplot(cleaned_df['age'],kde=True, bins=30, color='indigo')\n",
    "plt.title(\"Age Histogram\")\n",
    "plt.xlabel=('Age')\n",
    "plt.ylabel = ('Count')\n",
    "plt.show()\n",
    "\n",
    "#  Boxplot for 'balance'. Helps to identify the outliers or skewed distribution\n",
    "sns.boxplot(x=cleaned_df['balance'])\n",
    "plt.title(\"Distribution of Balance\")\n",
    "plt.xlabel='Balance'\n",
    "plt.ylabel = 'Count'\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obseve that balance amount around 100000 are outliers for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "numerical_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "correlation_matrix = cleaned_df[numerical_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From Correlation Matrix we can observe that, \n",
    " - pdays and previous = 0.45 \n",
    " - day and campaign = 0.16\n",
    " - age and balance = 0.098\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pmulti varibale plot for 'balance'. Helps to identify the outliers or skewed distribution\n",
    "clientgraph = sns.scatterplot(data = df,\n",
    "                              x =  \n",
    "                              hue = 'age')\n",
    "#plt.title(\"Age wise distribution of Balance\")\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze trends over time (by month).\n",
    "# Aggregate subscriptions by month\n",
    "monthly_data = df.groupby('month')['y'].value_counts().unstack()\n",
    "monthly_data.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.title(\"Monthly Subscriptions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Relationship between 'balance' and 'y'\n",
    "sns.boxplot(x=cleaned_df['y'], y=cleaned_df['balance'])\n",
    "plt.title(\"Balance vs Subscription to Term Deposit\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  line plot for 'balance'. Helps to identify the outliers or skewed distribution\n",
    "sns.countplot(data=df, x='age', hue='y')\n",
    "plt.title(\"Age vs Subscription\")\n",
    "plt.xticks(np.arange(0, 100, step=10))  # Set label locations.\n",
    "#plt.xticks(0,10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Frequency of 'job' types for each outcome\n",
    "sns.countplot(data=cleaned_df, x='job', hue='y')\n",
    "plt.title(\"Job vs Subscription\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Insights:\n",
    "# Understand which features might influence the target variable.\n",
    "# Identify trends or patterns (e.g., specific job types subscribing more).\n",
    "\n",
    "\n",
    "# Pairwise plots help visualize relationships between features.\n",
    "n1_cols = ['age','duration', 'campaign', 'pdays']\n",
    "n2_cols = ['age','balance', 'duration']\n",
    "sns.pairplot(cleaned_df, vars=n1_cols, hue='y', diag_kind=\"kde\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cleaned_df, vars=n2_cols, hue='y', diag_kind=\"kde\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers**\n",
    "\n",
    "Outliers are data points that differ significantly from the majority of the dataset. Identifying and handling outliers is crucial for ensuring the robustness of data analysis and machine learning models. Here's how you can identify and deal with outliers:\n",
    "\n",
    "Identifying Outliers\n",
    "1. Visualization Methods\n",
    "\n",
    "Box Plot: Displays the interquartile range (IQR) and highlights outliers beyond whiskers.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data['column_name'])\n",
    "plt.show()\n",
    "\n",
    "Scatter Plot: Useful for identifying outliers in two dimensions.\n",
    "\n",
    "plt.scatter(data['x_column'], data['y_column'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "2. Statistical Methods\n",
    "\n",
    "Z-Score Method:\n",
    "Calculates the standard deviation distance of a data point from the mean.\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "data['z_score'] = zscore(data['column_name'])\n",
    "outliers = data[data['z_score'].abs() > 3]  # Threshold: 3\n",
    "\n",
    "Interquartile Range (IQR):\n",
    "Values beyond \n",
    "Q1−1.5×IQR or \n",
    "Q3+1.5×IQR are considered outliers.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "Q1 = data['column_name'].quantile(0.25)\n",
    "Q3 = data['column_name'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data[(data['column_name'] < lower_bound) | (data['column_name'] > upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Remove vs. Retain Outliers**\n",
    "\n",
    "Remove: If outliers are due to measurement errors or are irrelevant to the problem.\n",
    "\n",
    "**Remove outliers based on IQR**\n",
    "\n",
    "cleaned_data = data[(data['balance'] >= lower_bound) & (data['balance'] <= upper_bound)]\n",
    "\n",
    "print(\"Cleaned data shape:\", cleaned_data.shape)\n",
    "\n",
    "\n",
    "Retain/Cap: If they carry valuable information, e.g., identifying fraud or anomalies.\n",
    "\n",
    "Option 2: Cap Outliers\n",
    "Cap extreme values at the IQR thresholds to reduce their influence.\n",
    "\n",
    "Cap outliers for 'balance'\n",
    "data['balance'] = data['balance'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "Option 3: Transform Data\n",
    "Apply a log transformation to normalize skewed data and reduce the effect of outliers.\n",
    "\n",
    "\\n python\n",
    "Copy code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data['log_balance'] = np.log1p(data['balance'])\n",
    "\n",
    "Use Robust Models: Algorithms like Random Forest or Gradient Boosting are less sensitive to outliers.\n",
    "\n",
    "Option 4: Use Robust Models\n",
    "\n",
    "Use models like Isolation Forest or DBSCAN for automatic outlier detection and removal.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.05)\n",
    "data['outlier'] = iso.fit_predict(data[['column_name']])\n",
    "data = data[data['outlier'] == 1]  # Keep only inliers\n",
    "\n",
    "\n",
    "If you're building a model, use algorithms like Random Forest or XGBoost that are less sensitive to outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we choose option :4 \n",
    "\n",
    "Use models like Isolation Forest or DBSCAN for automatic outlier detection and removal.\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.05)\n",
    "data['outlier'] = iso.fit_predict(data[['column_name']])\n",
    "data = data[data['outlier'] == 1]  # Keep only inliers\n",
    "\n",
    "                                OR\n",
    "                                \n",
    "We are going to use XGBoost and Random Forest algorithms for building model\n",
    "If you're building a model, use algorithms like Random Forest or XGBoost that are less sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Engineering\n",
    "\n",
    "Goal: Create new features or modify existing ones to improve model performance.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- Group numerical values (e.g., age groups).\n",
    "- Create interaction features (e.g., balance_per_campaign).\n",
    "- Encode categorical variables.(previously done)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize text formatting for categorical columns.**\n",
    "\n",
    "To standardize the categorical data in the bank-full.csv file, follow these steps:\n",
    "\n",
    "1. Separate Numerical and Categorical Columns\n",
    "Start by identifying which columns are numerical and which are categorical.\n",
    "2. Clean and Standardize Categorical Data\n",
    "    (a) Clean Values\n",
    "    Use string operations to ensure consistency:\n",
    "    for col in categorical_columns:\n",
    "        data_full_parsed[col] = data_full_parsed[col].str.lower().str.strip()\n",
    "\n",
    "    (b) Encode Categorical Values\n",
    "    \n",
    "        Depending on the type of categorical data:\n",
    "\n",
    "        1. Label Encoding for ordinal data:\n",
    "\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            for col in categorical_columns:\n",
    "             data_full_parsed[col] = le.fit_transform(data_full_parsed[col])\n",
    "\n",
    "\n",
    "        2. One-Hot Encoding for nominal data:\n",
    "\n",
    "        data_full_parsed = pd.get_dummies(data_full_parsed, columns=categorical_columns, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define categorical_columns\n",
    "cate_cols = ['job', 'marital', 'education', 'default', 'housing', 'contact', 'poutcome', 'loan', 'month', 'y']\n",
    "\n",
    "# Clean and encode categorical data\n",
    "for col in cate_cols:\n",
    "    df_cleaned[col] = df_cleaned[col].str.lower().str.strip()\n",
    "\n",
    "# Encode nominal categorical data (One-Hot Encoding)\n",
    "df_encoded = pd.get_dummies(df_cleaned, columns=cate_cols, drop_first=True)\n",
    "\n",
    "\n",
    "print(\"cleaned and categorical encoded data is ready for feature engineering : df_encoded \")\n",
    "df_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "#data = pd.get_dummies(data, columns=categorical_columns[:-1], drop_first=True)\n",
    "\n",
    "# converted categorical columns(True/False) in to numerical column(0/1)\n",
    "# Replace True and False with 1 and 0\n",
    "\n",
    "df_processed = df_encoded.replace({True : 1, False : 0})\n",
    "\n",
    "# Convert target variable (y) to binary\n",
    "#data['y'] = ['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "print(df_processed.info())\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Data Transformation\n",
    "\n",
    "Goal: Prepare the data for modeling by standardizing numerical features and handling imbalances in the target variable.\n",
    "\n",
    "Actions:\n",
    "- Scale numerical columns. (standardization of data and validation of standardized data)\n",
    "- Address class imbalance using techniques like SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the data in the bank-full.csv file, follow these steps:\n",
    "\n",
    "1. Standardize Numerical Columns\n",
    "Use StandardScaler from sklearn to scale numerical features:\n",
    "This step transforms the numerical features to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "2. Validate the Standardization\n",
    "After standardization, check that the numerical data has been scaled correctly and the categorical data is encoded:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the values (numerical and categorical)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Separate columns\n",
    "num_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "# Standardize numerical data\n",
    "scaler = StandardScaler()\n",
    "df_processed[num_cols] = scaler.fit_transform(df_processed[num_cols])\n",
    "\n",
    "\n",
    "# Validate changes\n",
    "\n",
    "print(df_processed.describe())  # For numerical columns\n",
    "#print(df_processed)     # To verify encoded categorical columns\n",
    "#print(df.size)\n",
    "print(df_processed.columns)\n",
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Saperate the features and output/target variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate X = features and y = target/output\n",
    "\n",
    "X = df_processed.drop('y_yes', axis =1)\n",
    "y = df_processed['y_yes']\n",
    "\n",
    "print(\"X features data :\", X)\n",
    "print(\"y target data :\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Imbalance:**\n",
    "\n",
    "Class imbalance occurs in a dataset when the distribution of classes in the target variable is significantly skewed, meaning one class is much more frequent than the other(s). This is common in many real-world scenarios, such as:\n",
    "\n",
    "Fraud detection (fraudulent transactions are rare compared to legitimate ones).\n",
    "Disease diagnosis (the number of patients with a specific disease may be much smaller than those without it).\n",
    "Customer churn (only a minority of customers may churn).\n",
    "For example, in a binary classification problem, if 90% of the data belongs to Class A and only 10% belongs to Class B, the dataset is imbalanced.\n",
    "\n",
    "Why Is Class Imbalance a Problem?\n",
    "Bias Toward the Majority Class: Many machine learning models optimize for overall accuracy, which may lead to predicting the majority class more often while neglecting the minority class.\n",
    "\n",
    "Example: A model predicting only Class A in the above example will achieve 90% accuracy but fail to identify any instances of Class B.\n",
    "Poor Performance Metrics: Metrics like accuracy become misleading since they do not consider the imbalance.\n",
    "\n",
    "Precision, recall, F1-score, and AUC-ROC are better alternatives in such cases.\n",
    "Impact on Model Learning:\n",
    "\n",
    "Models may fail to learn patterns for the minority class due to insufficient examples.\n",
    "The decision boundary may skew toward the majority class, reducing the model's ability to generalize.\n",
    "How to Detect Class Imbalance\n",
    "Inspect the distribution of the target variable:\n",
    "\n",
    "Code Example:\n",
    "\n",
    "**Check class distribution**\n",
    "print(data['target'].value_counts(normalize=True))\n",
    "Output:\n",
    "If the result shows something like:\n",
    "\n",
    "makefile\n",
    "Copy code\n",
    "0: 90%\n",
    "1: 10%\n",
    "You have an imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in the target variable 'y'\n",
    "print(df['y'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Interpretation:\n",
    "\n",
    "If one class (e.g., 'no' or 'yes') has significantly fewer samples, the dataset is imbalanced.\n",
    "Example: 90% 'no' and 10% 'yes'.\n",
    "\n",
    "For here 88% no and 12% yes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to Address Class Imbalance\n",
    "Resampling Techniques:\n",
    "\n",
    "- Oversampling the Minority Class: Duplicate or synthesize samples of the minority class (e.g., using    SMOTE - Synthetic Minority Oversampling Technique).\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "- Undersampling the Majority Class: Randomly remove samples from the majority class to balance the dataset.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersampler = RandomUnderSampler()\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "- Generate Synthetic Data: Use techniques like SMOTE or ADASYN to generate synthetic data points for the minority class.\n",
    "\n",
    "- Class Weighting: Assign higher weights to the minority class during training to penalize misclassifications more heavily.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "- Use Algorithms Designed for Imbalanced Data: Algorithms like XGBoost, LightGBM, or CatBoost have built-in parameters for handling class imbalance.\n",
    "\n",
    "Evaluation Metrics:\n",
    "\n",
    "Use metrics such as precision, recall, F1-score, AUC-ROC, or a confusion matrix instead of accuracy.\n",
    "For example:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "Ensemble Methods:\n",
    "\n",
    "Use methods like Balanced Random Forest or EasyEnsemble, which are designed for imbalanced datasets.\n",
    "Summary\n",
    "Class imbalance should not be ignored as it can skew model performance. By employing one or a combination of the above strategies, you can mitigate its effects and build a model that performs well across all classes. Would you like to address class imbalance for your dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approches to Choose a Strategy to Address Imbalance**\n",
    "\n",
    "**Option A: Oversampling the Minority Class**\n",
    "\n",
    "This strategy generates additional examples for the minority class. The most popular method is SMOTE.\n",
    "\n",
    "Code:\n",
    "python\n",
    "Copy code\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "- Separate features and target\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y'].apply(lambda x: 1 if x == 'yes' else 0)  # Convert target to binary\n",
    "\n",
    "- Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "- Check new class distribution\n",
    "from collections import Counter\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "\n",
    "\n",
    "When to Use:\n",
    "Use when the dataset is large enough, as oversampling increases data size.\n",
    "\n",
    "**Option B: Undersampling the Majority Class**\n",
    "\n",
    "This strategy removes samples from the majority class to balance the dataset.\n",
    "\n",
    "Code:\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "- Apply Random Under Sampler\n",
    "\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "- Check new class distribution\n",
    "\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "\n",
    "When to Use:\n",
    "Use when the dataset is large, and removing some majority class data won't impact model training.\n",
    "\n",
    "**Option C: Combine Over- and Undersampling**\n",
    "\n",
    "Balance the dataset using a combination of oversampling and undersampling.\n",
    "\n",
    "Code:\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "- Apply SMOTE + Edited Nearest Neighbors\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))\n",
    "\n",
    "**Option D: Assign Class Weights**\n",
    "\n",
    "Most classifiers support a class_weight parameter to handle imbalance by penalizing misclassification of the minority class.\n",
    "\n",
    "Code:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "- Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "- Train Random Forest with class weights\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "- Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "When to Use:\n",
    "Use when you prefer not to alter the dataset directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll implement SMOTE (Synthetic Minority Oversampling Technique) to oversample the minority class.\n",
    "\n",
    " This is an effective and commonly used method when you want to augment the minority class without losing any data.\n",
    "\n",
    "Here’s how we’ll handle this step by step:\n",
    "\n",
    "Step 1: Separate Features and Target\n",
    "We need to isolate the target variable y and the features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y'].apply(lambda x: 1 if x == 'yes' else 0)  # Convert target to binary (0 or 1)\n",
    "\n",
    "# Check the class distribution\n",
    "print(\"Class distribution before resampling:\")\n",
    "print(y.value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Apply SMOTE\n",
    "We’ll use the imblearn library to oversample the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the data\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"Class distribution after resampling:\")\n",
    "print(Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Results\n",
    "After completing the steps:\n",
    "\n",
    "Compare model performance before and after applying SMOTE.\n",
    "Check if the model improves its ability to predict the minority class (y='yes')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Impact**\n",
    "\n",
    "After applying your chosen strategy:\n",
    "\n",
    "Split the resampled dataset into training and testing sets.\n",
    "Train a machine learning model.\n",
    "\n",
    "Use metrics like precision, recall, F1-score, and ROC-AUC to evaluate performance.\n",
    "\n",
    "Code for Model Training and Evaluation:\n",
    "python\n",
    "Copy code\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "\n",
    "- Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n",
    "\n",
    "\n",
    "**Document Your Process**\n",
    "\n",
    "Explain the class imbalance issue and the strategy chosen.\n",
    "Justify the approach based on the dataset size and class distribution.\n",
    "Highlight model improvements after addressing imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# df_num[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "# Handle class imbalance\n",
    "X = df_processed.drop('y', axis=1)\n",
    "y = df_processed['y']\n",
    "smote = SMOTE()\n",
    "X, y = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Split the Data\n",
    "\n",
    "Goal: Divide the dataset into training and testing subsets.\n",
    "\n",
    "Actions:\n",
    "Split data into training (80%) and testing (20%).\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define categorical_columns\n",
    "cate_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'month', 'y']\n",
    "\n",
    "\n",
    "# Clean and encode categorical data\n",
    "for col in cate_cols:\n",
    "    cleaned_df[col] = cleaned_df[col].str.lower().str.strip()\n",
    "\n",
    "\n",
    "# Encode nominal categorical data (One-Hot Encoding)\n",
    "df_encoded = pd.get_dummies(cleaned_df, columns=cate_cols, drop_first=True)\n",
    "\n",
    "# Replace True and False with 1 and 0\n",
    "\n",
    "df_encoded = df_encoded.replace({True : 1, False : 0})\n",
    "\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing standardscaler for standardization of numeric columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which shows the size of our dataset df is 43193 rows and 36 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualization of initial data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for age \n",
    "sns.histplot(df_encoded['age'],kde=True, bins=30, color='indigo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Visualization:\n",
    "\n",
    "Goal: Represent data insights visually for better understanding.\n",
    "\n",
    "Key Actions:\n",
    "\n",
    "Use tools like Matplotlib, Seaborn, Power BI, or Tableau.\n",
    "Create charts, graphs, and dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions (e.g., histograms or boxplots)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Boxplot for 'balance'\n",
    "sns.boxplot(x=df['balance'])\n",
    "plt.title(\"Distribution of Balance\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
